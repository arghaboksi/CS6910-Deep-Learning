{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shubhammt/cs6910_assignment3/blob/main/seq2seq_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a29ff8f"
      },
      "source": [
        "# Question 1: Model Building"
      ],
      "id": "2a29ff8f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e50368c5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow \n",
        "import keras\n",
        "import io\n",
        "from math import log,log1p\n",
        "from numpy import array,argmax\n",
        "from keras.models import Model,load_model\n",
        "from keras.layers import Input, LSTM, Dense, Embedding, GRU, Dropout, SimpleRNN\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "id": "e50368c5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3e143481"
      },
      "outputs": [],
      "source": [
        "%pip install wandb -q\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback   "
      ],
      "id": "3e143481"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "150deed3",
        "outputId": "4b0edab5-2bf7-451e-900e-7200d3144cd8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "wandb.login()"
      ],
      "id": "150deed3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "98925165",
        "outputId": "34b323bb-978c-459c-81ba-e88c289b5f7a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href=\"https://wandb.me/wandb-init\" target=\"_blank\">the W&B docs</a>."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.15"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220501_100351-7zvv4yl6</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/shubham-argha/Assignment%203/runs/7zvv4yl6\" target=\"_blank\">rnn_beam_search_in_emb_32_num_enc_2_num_dec_2_dp_0.2_hidden_32</a></strong> to <a href=\"https://wandb.ai/shubham-argha/Assignment%203\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/shubham-argha/Assignment%203/sweeps/fmyurg9f\" target=\"_blank\">https://wandb.ai/shubham-argha/Assignment%203/sweeps/fmyurg9f</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/shubham-argha/Assignment%203/runs/7zvv4yl6?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f22704b3b90>"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "wandb.init(project=\"Assignment 3\", entity=\"shubham-argha\")"
      ],
      "id": "98925165"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPijA2bB35Ce"
      },
      "outputs": [],
      "source": [
        "class seq2seq(object):\n",
        "    def __init__(self,Type = 'rnn',input_embedding = 32, hid_layer_size = 32, lr= 1e-3, \n",
        "                   drop_prob=0.4,prediction ='greedy',number_of_epochs = 10, batch_size = 32,beam_width = 5,\n",
        "                   number_of_encoders = 1, number_of_decoders= 1):\n",
        "\n",
        "        self.Type = Type\n",
        "        self.input_embedding = input_embedding\n",
        "        self.hid_layer_size = hid_layer_size\n",
        "        self.lr = lr\n",
        "        self.drop_prob = drop_prob\n",
        "        self.prediction = prediction\n",
        "        self.number_of_epochs = number_of_epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.beam_width = beam_width\n",
        "        self.number_of_encoders = number_of_encoders\n",
        "        self.number_of_decoders = number_of_decoders\n",
        "\n",
        "    def fit(self,enc_inp, dec_inp, dec_tar,x_test, y_test):\n",
        "        # Input Sequence\n",
        "        enc_inps= Input(shape=(None, ),name = 'Enc_inputs')\n",
        "\n",
        "        # Embedding Layer\n",
        "        encoder_embedding=  Embedding(enc_tok_num, self.input_embedding , mask_zero = True,name = 'Enc_emb')(enc_inps)\n",
        "\n",
        "        enc_outs= encoder_embedding\n",
        "\n",
        "        if self.Type == 'lstm':\n",
        "\n",
        "            enc_LSTM = LSTM(self.hid_layer_size, return_state=True,dropout = self.drop_prob, return_sequences=True, name=\"Enc_hidden_1\")\n",
        "            enc_outs, h, c = enc_LSTM(enc_outs)\n",
        "            states= [h, c]\n",
        "\n",
        "            # LSTM Layer \n",
        "            for i in range( 2, self.number_of_encoders +1):\n",
        "                l= ('Enc_hidden_%d')%i\n",
        "                enc_LSTM = LSTM(self.hid_layer_size, return_state=True,dropout = self.drop_prob, return_sequences=True, name=l)\n",
        "                enc_outs, h, c = enc_LSTM(enc_outs,initial_state = states)\n",
        "                states = [h, c]\n",
        "\n",
        "        elif self.Type == 'gru':\n",
        "            enc_GRU = GRU(self.hid_layer_size, return_state=True,dropout = self.drop_prob, return_sequences=True, name=\"Enc_hidden_1\")\n",
        "            enc_outs, h = enc_GRU(enc_outs)\n",
        "            states = [h]\n",
        "\n",
        "            # GRU Layer\n",
        "            for i in range(2, self.number_of_encoders +1):\n",
        "                l = ('Enc_hidden_%d') %i\n",
        "                enc_GRU = GRU(self.hid_layer_size, return_state=True,dropout = self.drop_prob, return_sequences=True, name=l)\n",
        "                enc_outs, h = enc_GRU(enc_outs, initial_state = states)\n",
        "                states = [h]  \n",
        "\n",
        "        elif self.Type == 'rnn':\n",
        "            enc_rnn = SimpleRNN(self.hid_layer_size, return_state=True,dropout = self.drop_prob, return_sequences=True, name=\"Enc_hidden_1\")\n",
        "            enc_outs, h = enc_rnn(enc_outs)\n",
        "            states = [h]\n",
        "            \n",
        "            # RNN Layer\n",
        "            for i in range(2, self.number_of_encoders +1):\n",
        "                l = ('Enc_hidden_%d') %i\n",
        "                enc_rnn = SimpleRNN(self.hid_layer_size, return_state=True,dropout = self.drop_prob, return_sequences=True, name=l)\n",
        "                enc_outs, h = enc_rnn(enc_outs, initial_state = states)\n",
        "                states = [h]  \n",
        "\n",
        "        # Decoder uses \"encoder_states\" as initial state\n",
        "        dec_inps= Input(shape=(None,), name = 'Dec_inputs')\n",
        "        decoder_embedding_layer = Embedding(dec_tok_num, self.hid_layer_size, mask_zero = True, name = 'Dec_emb')\n",
        "        decoder_embedding = decoder_embedding_layer(dec_inps)\n",
        "        dec_outs = decoder_embedding\n",
        "\n",
        "        if self.Type == 'lstm':\n",
        "            dec_LSTM = LSTM(self.hid_layer_size, return_sequences=True, return_state=True,dropout = self.drop_prob, name=\"Dec_hidden_1\")\n",
        "            dec_outs, _, _ = dec_LSTM(dec_outs, initial_state = states)\n",
        "\n",
        "            for i in range(2, self.number_of_decoders +1):\n",
        "                l = ('Dec_hidden_%d') %i\n",
        "\n",
        "                dec_LSTM = LSTM(self.hid_layer_size, return_sequences=True, return_state=True,dropout = self.drop_prob, name=l)\n",
        "                dec_outs, _, _ = dec_LSTM(dec_outs, initial_state = states)\n",
        "\n",
        "        elif self.Type == 'gru':\n",
        "            dec_GRU = GRU(self.hid_layer_size, return_sequences=True, return_state=True,dropout = self.drop_prob, name=\"Dec_hidden_1\")\n",
        "            dec_outs, _ = dec_GRU(dec_outs, initial_state = states)\n",
        "\n",
        "            for i in range(2, self.number_of_decoders+1):\n",
        "                l = ('Dec_hidden_%d') %i\n",
        "                dec_GRU = GRU(self.hid_layer_size, return_sequences=True, return_state=True,dropout = self.drop_prob, name=l)\n",
        "                dec_outs, _ = dec_GRU(dec_outs, initial_state = states)\n",
        "\n",
        "        elif self.Type == 'rnn':\n",
        "            dec_RNN = SimpleRNN(self.hid_layer_size, return_sequences=True, return_state=True,dropout = self.drop_prob, name=\"Dec_hidden_1\")\n",
        "            dec_outs, _ = dec_RNN(dec_outs, initial_state = states)\n",
        "\n",
        "            for i in range(2, self.number_of_decoders+1):\n",
        "                l = ('Dec_hidden_%d') %i\n",
        "                dec_RNN = SimpleRNN(self.hid_layer_size, return_sequences=True, return_state=True,dropout = self.drop_prob, name=l)\n",
        "                dec_outs, _ = dec_RNN(dec_outs, initial_state = states)\n",
        "\n",
        "        den_dec= Dense(dec_tok_num, activation='softmax', name = 'dense')\n",
        "        dec_outs = den_dec(dec_outs)\n",
        "\n",
        "\n",
        "        # model takes encoder input and decoder input and outputs decoder output\n",
        "        model = Model([enc_inps, dec_inps], dec_outs)\n",
        "\n",
        "        model.summary()\n",
        "        \n",
        "        # Optimizer\n",
        "        optimizer = Adam(lr=self.lr, beta_1=0.9, beta_2=0.999)\n",
        "        model.compile(loss = \"categorical_crossentropy\", optimizer = optimizer, metrics=['accuracy'])\n",
        "\n",
        "        model.fit(\n",
        "            [enc_inp, dec_inp],\n",
        "            dec_tar,\n",
        "            batch_size=self.batch_size,\n",
        "            epochs=self.number_of_epochs,\n",
        "            callbacks = [WandbCallback()]\n",
        "            )\n",
        "\n",
        "\n",
        "        enc_mod, dec_mod = self.inference(model)\n",
        "        g_t = 0\n",
        "        g_c = 0\n",
        "\n",
        "        for i in range(len(val_src)):\n",
        "\n",
        "            input_sequence = x_test[i : i + 1]\n",
        "            rlt = self.dec_seq(enc_mod,dec_mod,input_sequence)\n",
        "            tgt = y_test[i]\n",
        "            tgt = tgt[1:len(tgt)-1]\n",
        "            rlt = rlt[0:len(rlt)-1]\n",
        "            \n",
        "            if rlt.strip() == tgt.strip():\n",
        "                g_c = g_c + 1\n",
        "\n",
        "            g_t = g_t + 1\n",
        "            accuracy_epoch = g_c/g_t\n",
        "\n",
        "            if g_t % 50 == 0:\n",
        "                wandb.log({'epoch_accuracy' : accuracy_epoch})\n",
        "            \n",
        "        val_accuracy = g_c/g_t\n",
        "        wandb.log({'val_accuracy' : val_accuracy})\n",
        "\n",
        "\n",
        "    def inference(self, model):\n",
        "\n",
        "        enc_inps = model.input[0]  \n",
        "\n",
        "        if self.Type == 'rnn' or self.Type == 'gru':\n",
        "            enc_outs, encoder_h = model.get_layer('Enc_hidden_'+ str(self.number_of_encoders)).output\n",
        "            states = [encoder_h]\n",
        "            enc_mod = Model(enc_inps, states)\n",
        "\n",
        "            dec_inps = model.input[1]  \n",
        "            dec_outs = model.get_layer('Dec_emb')(dec_inps)\n",
        "            inps_dec_states = []\n",
        "            dec_states = []\n",
        "\n",
        "            for i in range(1,self.number_of_decoders +1):\n",
        "                decoder_state_input_h = keras.Input(shape=(self.hid_layer_size,))\n",
        "                curr_states_inputs = [decoder_state_input_h]\n",
        "                decoder = model.get_layer('Dec_hidden_'+ str(i))\n",
        "                dec_outs, decoder_h = decoder(dec_outs, initial_state=curr_states_inputs)\n",
        "\n",
        "                dec_states += [decoder_h]\n",
        "                inps_dec_states += curr_states_inputs\n",
        "\n",
        "        elif self.Type == 'lstm':\n",
        "            enc_outs, encoder_h, encoder_c = model.get_layer('Enc_hidden_'+ str(self.number_of_encoders)).output \n",
        "            states = [encoder_h, encoder_c]\n",
        "            enc_mod = Model(enc_inps, states)\n",
        "\n",
        "            dec_inps = model.input[1] \n",
        "            dec_outs = model.get_layer('Dec_emb')(dec_inps)\n",
        "            inps_dec_states = []\n",
        "            dec_states = []\n",
        "\n",
        "            for i in range(1,self.number_of_decoders +1):\n",
        "                decoder_state_input_h = keras.Input(shape=(self.hid_layer_size,))\n",
        "                decoder_state_input_c = keras.Input(shape=(self.hid_layer_size,))\n",
        "                curr_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "                decoder = model.get_layer('Dec_hidden_'+ str(i))\n",
        "                dec_outs, decoder_h, decoder_c = decoder(dec_outs, initial_state=curr_states_inputs)\n",
        "\n",
        "                dec_states += [decoder_h, decoder_c]\n",
        "                inps_dec_states += curr_states_inputs\n",
        "\n",
        "\n",
        "        den_dec = model.get_layer('dense')\n",
        "        dec_outs = den_dec(dec_outs)\n",
        "        dec_mod = Model([dec_inps] + inps_dec_states, [dec_outs] + dec_states)\n",
        "\n",
        "        return enc_mod,dec_mod\n",
        "        \n",
        "\n",
        "    def search_beam(self,data, k):\n",
        "        seqs = [[list(), 0.0]]\n",
        "        for r in data:\n",
        "            a = list()\n",
        "            for i in range(len(seqs)):\n",
        "                seq, score = seqs[i]\n",
        "                for j in range(len(r)):\n",
        "                    c = [seq + [j], score - log(r[j])]\n",
        "                    a.append(c)\n",
        "        ord = sorted(a, key=lambda tup:tup[1])\n",
        "        seqs = ord[:k]\n",
        "        return seqs\n",
        "\n",
        "\n",
        "\n",
        "    def dec_seq(self,enc_mod,dec_mod,input_sequence):\n",
        "        #Input State Vectors\n",
        "        val_states = [enc_mod.predict(input_sequence)] * self.number_of_decoders\n",
        "\n",
        "        # Empty Target Sequence\n",
        "        tar_sequence = np.zeros((1, 1))\n",
        "        # Start Character\n",
        "        tar_sequence[0, 0] = tok_ind_tar['B']\n",
        "\n",
        "        # Sampling loop \n",
        "        flag = False\n",
        "        decoded_sentence = \"\"\n",
        "\n",
        "        while not flag:\n",
        "            if self.Type == 'rnn' or self.Type == 'gru':\n",
        "                d = dec_mod.predict([tar_sequence] + [val_states])\n",
        "                output_tokens, val_states = d[0],d[1:]\n",
        "\n",
        "            elif self.Type == 'lstm':  \n",
        "                d = dec_mod.predict([tar_sequence] + val_states)\n",
        "                output_tokens, val_states = d[0],d[1:]\n",
        "\n",
        "            if self.prediction == 'greedy':\n",
        "                width_beam = 1\n",
        "            elif self.prediction == 'beam_search':\n",
        "                width_beam = self.beam_width\n",
        "\n",
        "            sampled_token_index = self.search_beam(output_tokens[0,:,:], width_beam)\n",
        "            sampled_token_index = sampled_token_index[width_beam-1][0]\n",
        "\n",
        "            # Token Sampling\n",
        "            sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "            sampled_char = rtci[sampled_token_index]\n",
        "            decoded_sentence += sampled_char\n",
        "\n",
        "            # Exit condition\n",
        "            if sampled_char == 'E' or len(decoded_sentence) > max_decoder_seq_length:\n",
        "                flag = True\n",
        "\n",
        "            # tar_sequence update\n",
        "            tar_sequence = np.zeros((1, 1))\n",
        "            tar_sequence[0, 0] = sampled_token_index\n",
        "\n",
        "        return decoded_sentence\n"
      ],
      "id": "wPijA2bB35Ce"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47a96e20"
      },
      "source": [
        "# Question 2: Dataset and training"
      ],
      "id": "47a96e20"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "606c021c"
      },
      "outputs": [],
      "source": [
        "train_dir = \"../dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\"\n",
        "dev_dir = \"../dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\"\n",
        "test_dir = \"../dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\""
      ],
      "id": "606c021c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibKpb8zcfEw-",
        "outputId": "f508459e-4829-4e0e-9e13-9bc678447e6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-01 10:03:58--  https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.119.128, 108.177.126.128, 108.177.127.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.119.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2008340480 (1.9G) [application/x-tar]\n",
            "Saving to: ‘dakshina_dataset_v1.0.tar.1’\n",
            "\n",
            "dakshina_dataset_v1 100%[===================>]   1.87G   128MB/s    in 25s     \n",
            "\n",
            "2022-05-01 10:04:24 (75.2 MB/s) - ‘dakshina_dataset_v1.0.tar.1’ saved [2008340480/2008340480]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Dataset\n",
        "!yes | wget \"https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\""
      ],
      "id": "ibKpb8zcfEw-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjPGy-Z3fE5n"
      },
      "outputs": [],
      "source": [
        "# Unzip\n",
        "!yes | tar xopf dakshina_dataset_v1.0.tar"
      ],
      "id": "AjPGy-Z3fE5n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iiZQH9QfNvV",
        "outputId": "06c93994-b91e-409b-ac90-4d93d111792a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hi.translit.sampled.dev.tsv   hi.translit.sampled.train.tsv\n",
            "hi.translit.sampled.test.tsv\n"
          ]
        }
      ],
      "source": [
        "# Train, Test and Validation Data\n",
        "!ls dakshina_dataset_v1.0/hi/lexicons"
      ],
      "id": "0iiZQH9QfNvV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3mpA6t4fiRQ"
      },
      "outputs": [],
      "source": [
        "train_dir = \"./dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\"\n",
        "dev_dir = \"./dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\"\n",
        "test_dir = \"./dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\""
      ],
      "id": "L3mpA6t4fiRQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59276c91"
      },
      "outputs": [],
      "source": [
        "# reads text document and returns a list of lists comprising the romanized and native versions of the words\n",
        "def read(f):\n",
        "    hindi = []\n",
        "    latin = []\n",
        "    with io.open(f, encoding ='utf-8') as f:\n",
        "        for line in f:\n",
        "            if '\\t' not in line:\n",
        "                continue\n",
        "            tokens = line.rstrip().split(\"\\t\")\n",
        "            latin.append(tokens[1])\n",
        "            hindi.append(tokens[0])\n",
        "    return latin, hindi"
      ],
      "id": "59276c91"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gECfo0ublt29"
      },
      "outputs": [],
      "source": [
        "tr_src, tr_tar = read(train_dir)\n",
        "val_src, val_tar = read(dev_dir)\n",
        "test_source, test_target = read(test_dir)"
      ],
      "id": "gECfo0ublt29"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hP9m3zjMluaE",
        "outputId": "f3af46fb-2ca6-4eff-c78b-997165fa9dd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training samples:  44204\n",
            "Number of validation samples:  4358\n",
            "Number of testing samples:  4502\n"
          ]
        }
      ],
      "source": [
        "print(\"Number of training samples: \", len(tr_src))\n",
        "print(\"Number of validation samples: \", len(val_src))\n",
        "print(\"Number of testing samples: \", len(test_source))"
      ],
      "id": "hP9m3zjMluaE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "db05e2b2"
      },
      "outputs": [],
      "source": [
        "arr = np.arange(len(tr_src))\n",
        "np.random.shuffle(arr)\n",
        "arr1 = np.arange(len(val_src))\n",
        "np.random.shuffle(arr1)"
      ],
      "id": "db05e2b2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJ7mELE_wnbp"
      },
      "outputs": [],
      "source": [
        "char_inp = set()\n",
        "char_tar = set()\n",
        "ns_txt_inp = []\n",
        "ns_txt_tar = []\n",
        "ns_txt_inp_val = []\n",
        "ns_txt_tar_val = []"
      ],
      "id": "iJ7mELE_wnbp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "um-PoQlIwuit"
      },
      "outputs": [],
      "source": [
        "for (txt_inp, txt_tar) in zip(tr_src, tr_tar):\n",
        "    # tab : \"start sequence\" character\n",
        "    # \\n  : \"end sequence\" character\n",
        "    txt_tar = \"B\" + txt_tar + \"E\"\n",
        "    ns_txt_inp.append(txt_inp)\n",
        "    ns_txt_tar.append(txt_tar)\n",
        "\n",
        "    for char in txt_inp:\n",
        "        if char not in char_inp:\n",
        "            char_inp.add(char)\n",
        "\n",
        "    for char in txt_tar:\n",
        "        if char not in char_tar:\n",
        "            char_tar.add(char)\n",
        "\n",
        "\n",
        "for (txt_inp, txt_tar) in zip(val_src, val_tar):\n",
        "    # tab : \"start sequence\" character\n",
        "    # \\n  : \"end sequence\" character\n",
        "    txt_tar = \"B\" + txt_tar + \"E\"\n",
        "    ns_txt_inp_val.append(txt_inp)\n",
        "    ns_txt_tar_val.append(txt_tar)\n",
        "    for char in txt_inp:\n",
        "        if char not in char_inp:\n",
        "            char_inp.add(char)\n",
        "    for char in txt_tar:\n",
        "        if char not in char_tar:\n",
        "            char_tar.add(char)"
      ],
      "id": "um-PoQlIwuit"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODBmuCpJw0wB"
      },
      "outputs": [],
      "source": [
        "inps_txt = []\n",
        "tars_txt = []\n",
        "\n",
        "for i in range(len(tr_src)):\n",
        "    inps_txt.append(ns_txt_inp[arr[i]])\n",
        "    tars_txt.append(ns_txt_tar[arr[i]])\n",
        "\n",
        "val_input_texts = []\n",
        "val_target_texts = []\n",
        "\n",
        "for i in range(len(val_src)):\n",
        "    val_input_texts.append(ns_txt_inp_val[arr1[i]])\n",
        "    val_target_texts.append(ns_txt_tar_val[arr1[i]])"
      ],
      "id": "ODBmuCpJw0wB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UplfBbbTw8SA"
      },
      "outputs": [],
      "source": [
        "char_inp.add(\" \")\n",
        "char_tar.add(\" \")\n",
        "\n",
        "char_inp = sorted(list(char_inp))\n",
        "char_tar = sorted(list(char_tar))\n",
        "\n",
        "enc_tok_num = len(char_inp)\n",
        "dec_tok_num= len(char_tar)\n",
        "max_encoder_seq_length = max([len(txt) for txt in inps_txt])\n",
        "max_decoder_seq_length = max([len(txt) for txt in tars_txt])\n",
        "val_max_encoder_seq_length = max([len(txt) for txt in val_input_texts])\n",
        "val_max_decoder_seq_length = max([len(txt) for txt in val_target_texts])"
      ],
      "id": "UplfBbbTw8SA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNhhS_GSw9l1",
        "outputId": "c2220881-6b20-4c62-96d5-a5ad5fb1ffc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples: 44204\n",
            "Number of unique input tokens: 27\n",
            "Number of unique output tokens: 66\n",
            "Max sequence length for inputs: 20\n",
            "Max sequence length for outputs: 21\n",
            "Max sequence length for val inputs: 18\n",
            "Max sequence length for val outputs: 16\n",
            "[' ', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
            "[' ', 'B', 'E', 'ँ', 'ं', 'ः', 'अ', 'आ', 'इ', 'ई', 'उ', 'ऊ', 'ऋ', 'ए', 'ऐ', 'ऑ', 'ओ', 'औ', 'क', 'ख', 'ग', 'घ', 'ङ', 'च', 'छ', 'ज', 'झ', 'ञ', 'ट', 'ठ', 'ड', 'ढ', 'ण', 'त', 'थ', 'द', 'ध', 'न', 'प', 'फ', 'ब', 'भ', 'म', 'य', 'र', 'ल', 'व', 'श', 'ष', 'स', 'ह', '़', 'ा', 'ि', 'ी', 'ु', 'ू', 'ृ', 'ॅ', 'े', 'ै', 'ॉ', 'ो', 'ौ', '्', 'ॐ']\n"
          ]
        }
      ],
      "source": [
        "print(\"Number of samples:\", len(inps_txt))\n",
        "print(\"Number of unique input tokens:\", enc_tok_num)\n",
        "print(\"Number of unique output tokens:\", dec_tok_num)\n",
        "print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
        "print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n",
        "print(\"Max sequence length for val inputs:\", val_max_encoder_seq_length)\n",
        "print(\"Max sequence length for val outputs:\", val_max_decoder_seq_length)\n",
        "print(char_inp)\n",
        "print(char_tar)"
      ],
      "id": "pNhhS_GSw9l1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93abca5e",
        "outputId": "2ebf44de-cada-43fd-c206-b4898eb3fe4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['jansaadharan', 'konkan', 'illingworth', 'pritish', 'bauddha', 'jaidi', 'dabav']\n",
            "['BजनसाधारणE', 'BकोंकणE', 'Bइलिंगवर्थE', 'Bप्रीतिशE', 'Bबौद्धE', 'BजैदीE', 'BदबावE']\n"
          ]
        }
      ],
      "source": [
        "print(inps_txt[123:130])\n",
        "print(tars_txt[123:130])"
      ],
      "id": "93abca5e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9002470"
      },
      "outputs": [],
      "source": [
        "tok_ind_inp= dict([(char, i) for i, char in enumerate(char_inp)])\n",
        "tok_ind_tar= dict([(char, i) for i, char in enumerate(char_tar)])"
      ],
      "id": "f9002470"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8Um008X2B6-",
        "outputId": "e5f28746-8ec9-41fd-9de7-f678eb67914d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{' ': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n",
            "{' ': 0, 'B': 1, 'E': 2, 'ँ': 3, 'ं': 4, 'ः': 5, 'अ': 6, 'आ': 7, 'इ': 8, 'ई': 9, 'उ': 10, 'ऊ': 11, 'ऋ': 12, 'ए': 13, 'ऐ': 14, 'ऑ': 15, 'ओ': 16, 'औ': 17, 'क': 18, 'ख': 19, 'ग': 20, 'घ': 21, 'ङ': 22, 'च': 23, 'छ': 24, 'ज': 25, 'झ': 26, 'ञ': 27, 'ट': 28, 'ठ': 29, 'ड': 30, 'ढ': 31, 'ण': 32, 'त': 33, 'थ': 34, 'द': 35, 'ध': 36, 'न': 37, 'प': 38, 'फ': 39, 'ब': 40, 'भ': 41, 'म': 42, 'य': 43, 'र': 44, 'ल': 45, 'व': 46, 'श': 47, 'ष': 48, 'स': 49, 'ह': 50, '़': 51, 'ा': 52, 'ि': 53, 'ी': 54, 'ु': 55, 'ू': 56, 'ृ': 57, 'ॅ': 58, 'े': 59, 'ै': 60, 'ॉ': 61, 'ो': 62, 'ौ': 63, '्': 64, 'ॐ': 65}\n"
          ]
        }
      ],
      "source": [
        "print(tok_ind_inp)\n",
        "print(tok_ind_tar)"
      ],
      "id": "e8Um008X2B6-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d24a9015"
      },
      "outputs": [],
      "source": [
        "enc_inp = np.zeros(\n",
        "    (len(inps_txt), max_encoder_seq_length), dtype=\"float32\"\n",
        ")\n",
        "dec_inp = np.zeros(\n",
        "    (len(inps_txt), max_decoder_seq_length), dtype=\"float32\"\n",
        ")\n",
        "dec_tar = np.zeros(\n",
        "    (len(inps_txt), max_decoder_seq_length, dec_tok_num), dtype=\"float32\"\n",
        ")"
      ],
      "id": "d24a9015"
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (txt_inp, txt_tar) in enumerate(zip(inps_txt, tars_txt)):\n",
        "    \n",
        "    for t, char in enumerate(txt_inp):\n",
        "        enc_inp[i, t] = tok_ind_inp[char]\n",
        "    enc_inp[i, t + 1 :] = tok_ind_inp[\" \"]\n",
        "\n",
        "    for t, char in enumerate(txt_tar):\n",
        "\n",
        "        dec_inp[i, t] = tok_ind_tar[char]\n",
        "\n",
        "        if t > 0:\n",
        "            dec_tar[i, t - 1, tok_ind_tar[char]] = 1.0\n",
        "\n",
        "    dec_inp[i, t + 1: ] = tok_ind_tar[\" \"]\n",
        "    dec_tar[i, t:, tok_ind_tar[\" \"]] = 1.0"
      ],
      "metadata": {
        "id": "fgHzGao4AyiQ"
      },
      "id": "fgHzGao4AyiQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp_enc_val = np.zeros(\n",
        "    (len(inps_txt), val_max_encoder_seq_length), dtype=\"float32\"\n",
        ")\n",
        "inp_dec_val= np.zeros(\n",
        "    (len(inps_txt), val_max_decoder_seq_length), dtype=\"float32\"\n",
        ")\n",
        "tar_dec_val = np.zeros(\n",
        "    (len(inps_txt), val_max_decoder_seq_length, dec_tok_num), dtype=\"float32\"\n",
        ")"
      ],
      "metadata": {
        "id": "OCg17I0CBUwR"
      },
      "id": "OCg17I0CBUwR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (txt_inp, txt_tar) in enumerate(zip(val_input_texts, val_target_texts)):\n",
        "    \n",
        "    for t, char in enumerate(txt_inp):\n",
        "        inp_enc_val[i, t] = tok_ind_inp[char]\n",
        "    inp_enc_val[i, t + 1 :] = tok_ind_inp[\" \"]\n",
        "\n",
        "    for t, char in enumerate(txt_tar):\n",
        "        inp_dec_val[i, t] = tok_ind_tar[char]\n",
        "\n",
        "        if t > 0:\n",
        "            tar_dec_val[i, t - 1, tok_ind_tar[char]] = 1.0\n",
        "            \n",
        "    inp_dec_val[i, t + 1: ] = tok_ind_tar[\" \"]\n",
        "    tar_dec_val[i, t:, tok_ind_tar[\" \"]] = 1.0"
      ],
      "metadata": {
        "id": "gooub-8bBYe1"
      },
      "id": "gooub-8bBYe1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1c161994"
      },
      "outputs": [],
      "source": [
        "rtci= dict((i, char) for char, i in tok_ind_tar.items())"
      ],
      "id": "1c161994"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fe7b8ab2"
      },
      "outputs": [],
      "source": [
        "x_test = inp_enc_val\n",
        "y_test = val_target_texts"
      ],
      "id": "fe7b8ab2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2f6aebad"
      },
      "outputs": [],
      "source": [
        "sweep_configuration = {\n",
        "    'method': 'bayes', \n",
        "    'metric': {\n",
        "      'name': 'val_accuracy',\n",
        "      'goal': 'maximize'   \n",
        "    },\n",
        "    'parameters': {\n",
        "\n",
        "        'drop_prob': {\n",
        "            'values': [0.2,0.3]\n",
        "        },\n",
        "        'lr': {\n",
        "            'values': [1e-3, 1e-4]\n",
        "        },\n",
        "        'batch_size': {\n",
        "            'values': [64, 128]\n",
        "        },\n",
        "        'input_embedding': {\n",
        "            'values': [16,32, 64,128,256]\n",
        "        },\n",
        "        'number_of_encoders': {\n",
        "            'values': [1, 2, 3]\n",
        "        },\n",
        "        'number_of_decoders': {\n",
        "            'values': [1, 2, 3]\n",
        "        },\n",
        "        'hid_layer_size':{\n",
        "            'values': [16,32, 64, 128,256]\n",
        "        },\n",
        "        'Type': {\n",
        "            'values': ['rnn', 'gru', 'lstm']\n",
        "        },\n",
        "        'dec_search': {\n",
        "            'values': ['beam_search', 'greedy']\n",
        "        },\n",
        "        'beam_width':{\n",
        "            'values': [3,5]\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "id": "2f6aebad"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b48918f",
        "outputId": "e6ed793c-7ce3-4748-8ba7-4239bb84b2f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: doww1p5f\n",
            "Sweep URL: https://wandb.ai/shubham-argha/Assignment%203/sweeps/doww1p5f\n"
          ]
        }
      ],
      "source": [
        "sweep_id = wandb.sweep(sweep_configuration, entity=\"shubham-argha\", project=\"Assignment 3\")"
      ],
      "id": "5b48918f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0585d65"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "        config_defaults = {\n",
        "            'drop_prob': 0.4,\n",
        "            'lr': 1e-3,\n",
        "            'batch_size': 32,\n",
        "            'number_of_epochs' : 15,\n",
        "            'input_embedding': 32,\n",
        "            'number_of_encoders': 2,\n",
        "            'number_of_decoders': 2,\n",
        "            'hid_layer_size': 32,\n",
        "            'Type': 'rnn',\n",
        "            'dec_search': 'beam_search',\n",
        "            'beam_width': 5\n",
        "            }\n",
        "\n",
        "        wandb.init(config = config_defaults)\n",
        "        config = wandb.config\n",
        "        wandb.run.name = str(config.Type)+ '_' + config.dec_search+'_in_emb_'+str(config.input_embedding)+'_num_enc_'+str(config.number_of_encoders)+'_num_dec_'+str(config.number_of_decoders)+'_dp_'+str(config.drop_prob)+'_hidden_'+str(config.hid_layer_size)\n",
        "\n",
        "        model_rnn = seq2seq(Type = config.Type, input_embedding = config.input_embedding, hid_layer_size=config.hid_layer_size,\n",
        "                    lr= config.lr, drop_prob=config.drop_prob,prediction = config.dec_search,number_of_epochs = config.number_of_epochs,\n",
        "                    batch_size = config.batch_size, beam_width = config.beam_width, number_of_encoders = config.number_of_encoders,number_of_decoders = config.number_of_decoders)\n",
        "\n",
        "        model_rnn.fit(enc_inp,dec_inp,dec_tar,x_test, y_test)"
      ],
      "id": "c0585d65"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        },
        "id": "e6f2d99b",
        "outputId": "1b2313ea-557c-4009-e1c5-769ce431af40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ambyaiaw with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tType: gru\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_width: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_search: greedy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdrop_prob: 0.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tin_emb: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_dec: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_enc: 2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.15"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220501_100439-ambyaiaw</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/shubham-argha/Assignment%203/runs/ambyaiaw\" target=\"_blank\">glad-sweep-36</a></strong> to <a href=\"https://wandb.ai/shubham-argha/Assignment%203\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/shubham-argha/Assignment%203/sweeps/fmyurg9f\" target=\"_blank\">https://wandb.ai/shubham-argha/Assignment%203/sweeps/fmyurg9f</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " Enc_inputs (InputLayer)        [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " Enc_emb (Embedding)            (None, None, 32)     864         ['Enc_inputs[0][0]']             \n",
            "                                                                                                  \n",
            " Dec_inputs (InputLayer)        [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " Enc_hidden_1 (GRU)             [(None, None, 32),   6336        ['Enc_emb[0][0]']                \n",
            "                                 (None, 32)]                                                      \n",
            "                                                                                                  \n",
            " Dec_emb (Embedding)            (None, None, 32)     2112        ['Dec_inputs[0][0]']             \n",
            "                                                                                                  \n",
            " Enc_hidden_2 (GRU)             [(None, None, 32),   6336        ['Enc_hidden_1[0][0]',           \n",
            "                                 (None, 32)]                      'Enc_hidden_1[0][1]']           \n",
            "                                                                                                  \n",
            " Dec_hidden_1 (GRU)             [(None, None, 32),   6336        ['Dec_emb[0][0]',                \n",
            "                                 (None, 32)]                      'Enc_hidden_2[0][1]']           \n",
            "                                                                                                  \n",
            " Dec_hidden_2 (GRU)             [(None, None, 32),   6336        ['Dec_hidden_1[0][0]',           \n",
            "                                 (None, 32)]                      'Enc_hidden_2[0][1]']           \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, None, 66)     2178        ['Dec_hidden_2[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 30,498\n",
            "Trainable params: 30,498\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "691/691 [==============================] - 53s 54ms/step - loss: 1.3631 - accuracy: 0.1632 - _timestamp: 1651399542.0000 - _runtime: 63.0000\n",
            "Epoch 2/15\n",
            "691/691 [==============================] - 37s 54ms/step - loss: 1.1862 - accuracy: 0.2343 - _timestamp: 1651399579.0000 - _runtime: 100.0000\n",
            "Epoch 3/15\n",
            "572/691 [=======================>......] - ETA: 6s - loss: 1.1084 - accuracy: 0.3030"
          ]
        }
      ],
      "source": [
        "wandb.agent('fmyurg9f',train, count = 1)"
      ],
      "id": "e6f2d99b"
    }
  ],
  "metadata": {
    "colab": {
      "name": "seq2seq_model.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}